{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (42000, 785)\n",
      "test shape: (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Relevant imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Extract MNIST csv data into train & test variables\n",
    "train = np.array(pd.read_csv('train.csv', delimiter=','))\n",
    "test = np.array(pd.read_csv('test.csv', delimiter=','))\n",
    "\n",
    "# Check shape of train & test datasets\n",
    "print(f'train shape: {train.shape}')\n",
    "print(f'test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Extract the first column of the training dataset into a label array\n",
    "label = train[:,0]\n",
    "# The train dataset now becomes all columns except the first\n",
    "train = train[:,1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Value: 5\n",
      "Corresponding y vector: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Initialise vector of all zeroes with 10 columns and the same number of\n",
    "# rows as the label array\n",
    "Y = np.zeros((label.shape[0], 10))\n",
    "\n",
    "# assign a value of 1 to each column index matching the label value\n",
    "Y[np.arange(0,label.shape[0]), label] = 1.0\n",
    "\n",
    "print(f'Label Value: {label[8]}')\n",
    "print(f'Corresponding y vector: {Y[8]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Normalize test & training dataset\n",
    "train = train / 255\n",
    "test = test / 255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "# def softmax(X):\n",
    "#     return np.exp(X)/sum(np.exp(X))\n",
    "\n",
    "# stable softmax\n",
    "def softmax(X):\n",
    "    Z = X - max(X)\n",
    "    numerator = np.exp(Z)\n",
    "    denominator = np.sum(numerator)\n",
    "    return numerator/denominator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Calculates the output of a given layer\n",
    "def calculate_layer_output(w, prev_layer_output, b, activation_type=\"relu\"):\n",
    "    # Steps 1 & 2\n",
    "    g = w @ prev_layer_output + b\n",
    "\n",
    "    # Step 3\n",
    "    if activation_type == \"relu\":\n",
    "        return relu(g)\n",
    "    if activation_type == \"softmax\":\n",
    "        return softmax(g)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Initialize weights & biases\n",
    "def init_layer_params(row, col):\n",
    "    w = np.random.randn(row, col)\n",
    "    b = np.random.randn(row, 1)\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Value: 5\n",
      "Prediction: 4\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights & baises for each layer\n",
    "w1, b1 = init_layer_params(10, 784)\n",
    "w2, b2 = init_layer_params(10, 10)\n",
    "w3, b3 = init_layer_params(10, 10)\n",
    "\n",
    "# Forward Pass through the Neural Network to make prediction\n",
    "input = train[8:9].T # Make the input a single image vector\n",
    "h1 = calculate_layer_output(w1, input, b1, activation_type=\"relu\")\n",
    "h2 = calculate_layer_output(w2, h1, b2, activation_type=\"relu\")\n",
    "output = calculate_layer_output(w3, h2, b3, activation_type=\"softmax\")\n",
    "\n",
    "print(f'Label Value: {label[8]}')\n",
    "print(f'Prediction: {np.argmax(output)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Calculate ReLU derivative\n",
    "def relu_derivative(g):\n",
    "    derivative = g.copy()\n",
    "    derivative[derivative<=0] = 0\n",
    "    derivative[derivative>0] = 1\n",
    "    return np.diag(derivative.T[0])\n",
    "\n",
    "# Calculate Softmax derivative\n",
    "def softmax_derivative(o):\n",
    "    derivative = np.diag(o.T[0])\n",
    "\n",
    "    for i in range(len(derivative)):\n",
    "        for j in range(len(derivative)):\n",
    "            if i == j:\n",
    "                derivative[i][j] = o[i] * (1 - o[i])\n",
    "            else:\n",
    "                derivative[i][j] = -o[i] * o[j]\n",
    "    return derivative"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def layer_backprop(previous_derivative, layer_output, previous_layer_output\n",
    "                   , w, activation_type=\"relu\"):\n",
    "    # 1. Calculate the derivative of the activation func\n",
    "    dh_dg = None\n",
    "    if activation_type == \"relu\":\n",
    "        dh_dg = relu_derivative(layer_output)\n",
    "    elif activation_type == \"softmax\":\n",
    "        dh_dg = softmax_derivative(layer_output)\n",
    "\n",
    "    # 2. Apply chain rule to get derivative of Loss function with respect to:\n",
    "    dL_dg = dh_dg @ previous_derivative # activation function\n",
    "\n",
    "    # 3. Calculate the derivative of the linear function with respect to:\n",
    "    dg_dw = previous_layer_output.T     # a) weight matrix\n",
    "    dg_dh = w.T                         # b) previous layer output\n",
    "    dg_db = 1.0                         # c) bias vector\n",
    "\n",
    "    # 4. Apply chain rule to get derivative of Loss function with respect to:\n",
    "    dL_dw = dL_dg @ dg_dw               # a) weight matrix\n",
    "    dL_dh = dg_dh @ dL_dg               # b) previous layer output\n",
    "    dL_db = dL_dg * dg_db               # c) bias vector\n",
    "\n",
    "    return dL_dw, dL_dh, dL_db"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def gradient_descent(w, b, dL_dw, dL_db, learning_rate):\n",
    "    w -= learning_rate * dL_dw\n",
    "    b -= learning_rate * dL_db\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_prediction(o):\n",
    "    return np.argmax(o)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Compute Accuracy (%) across all training data\n",
    "def compute_accuracy(train, label, w1, b1, w2, b2, w3, b3):\n",
    "    # Set params\n",
    "    correct = 0\n",
    "    total = train.shape[0]\n",
    "\n",
    "    # Iterate through training data\n",
    "    for index in range(0, total):\n",
    "        # Select a single data point (image)\n",
    "        X = train[index: index+1,:].T\n",
    "\n",
    "        # Forward pass: compute Output/Prediction (o)\n",
    "        h1 = calculate_layer_output(w1, X, b1, activation_type=\"relu\")\n",
    "        h2 = calculate_layer_output(w2, h1, b2, activation_type=\"relu\")\n",
    "        o = calculate_layer_output(w3, h2, b3, activation_type=\"softmax\")\n",
    "\n",
    "        # If prediction matches label Increment correct count\n",
    "        if label[index] == get_prediction(o):\n",
    "            correct+=1\n",
    "\n",
    "    # Return Accuracy (%)\n",
    "    return (correct / total) * 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Epoch 0 -------------\n",
      "Accuracy: 35.46 %\n",
      "------------- Epoch 1 -------------\n",
      "Accuracy: 50.10 %\n",
      "------------- Epoch 2 -------------\n",
      "Accuracy: 56.46 %\n",
      "------------- Epoch 3 -------------\n",
      "Accuracy: 60.20 %\n",
      "------------- Epoch 4 -------------\n",
      "Accuracy: 63.30 %\n",
      "------------- Epoch 5 -------------\n",
      "Accuracy: 67.20 %\n",
      "------------- Epoch 6 -------------\n",
      "Accuracy: 70.09 %\n",
      "------------- Epoch 7 -------------\n",
      "Accuracy: 72.77 %\n",
      "------------- Epoch 8 -------------\n",
      "Accuracy: 75.17 %\n",
      "------------- Epoch 9 -------------\n",
      "Accuracy: 77.04 %\n",
      "------------- Epoch 10 -------------\n",
      "Accuracy: 69.76 %\n",
      "------------- Epoch 11 -------------\n",
      "Accuracy: 77.68 %\n",
      "------------- Epoch 12 -------------\n",
      "Accuracy: 79.26 %\n",
      "------------- Epoch 13 -------------\n",
      "Accuracy: 79.25 %\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameter(s)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Set other params\n",
    "epoch = 0\n",
    "previous_accuracy = 100\n",
    "accuracy = 0\n",
    "\n",
    "# Randomly initialize weights & biases\n",
    "w1, b1 = init_layer_params(10, 784)   # Hidden Layer 1\n",
    "w2, b2 = init_layer_params(10, 10)    # Hidden Layer 2\n",
    "w3, b3 = init_layer_params(10, 10)    # Output Layer\n",
    "\n",
    "# While:\n",
    "#  1. Accuracy is improving by 1% or more per epoch, and\n",
    "#  2. There are 20 epochs or less\n",
    "while abs(accuracy - previous_accuracy) >= 1 and epoch <= 20:\n",
    "    print(f'------------- Epoch {epoch} -------------')\n",
    "\n",
    "    # record previous accuracy\n",
    "    previous_accuracy = accuracy\n",
    "\n",
    "    # Iterate through training data\n",
    "    for index in range(0, train.shape[0]):\n",
    "        # Select a single image and associated y vector\n",
    "        X = train[index:index+1,:].T\n",
    "        y = Y[index:index+1].T\n",
    "\n",
    "        # 1. Forward pass: compute Output/Prediction (o)\n",
    "        h1 = calculate_layer_output(w1, X, b1, activation_type=\"relu\")\n",
    "        h2 = calculate_layer_output(w2, h1, b2, activation_type=\"relu\")\n",
    "        o = calculate_layer_output(w3, h2, b3, activation_type=\"softmax\")\n",
    "\n",
    "        # 2. Compute Loss Vector\n",
    "        L = np.square(o - y)\n",
    "\n",
    "        # 3. Backpropagation\n",
    "        # Compute Loss derivative w.r.t. Output/Prediction vector (o)\n",
    "        dL_do = 2.0 * (o - y)\n",
    "\n",
    "        # Compute Output Layer derivatives\n",
    "        dL3_dw3, dL3_dh2, dL3_db3 = layer_backprop(dL_do, o, h2, w3, \"softmax\")\n",
    "        # Compute Hidden Layer 2 derivatives\n",
    "        dL2_dw2, dL2_dh2, dL2_db2 = layer_backprop(dL3_dh2, h2, h1, w2, \"relu\")\n",
    "        # Compute Hidden Layer 1 derivatives\n",
    "        dL1_dw1, _, dL1_db1 = layer_backprop(dL2_dh2, h1, X, w1, \"relu\")\n",
    "\n",
    "        # 4. Update weights & biases\n",
    "        w1, b1 = gradient_descent(w1, b1, dL1_dw1, dL1_db1, learning_rate)\n",
    "        w2, b2 = gradient_descent(w2, b2, dL2_dw2, dL2_db2, learning_rate)\n",
    "        w3, b3 = gradient_descent(w3, b3, dL3_dw3, dL3_db3, learning_rate)\n",
    "\n",
    "\n",
    "    # Compute & print Accuracy (%)\n",
    "    accuracy = compute_accuracy(train, label, w1, b1, w2, b2, w3, b3)\n",
    "    print(f'Accuracy: {accuracy:.2f} %')\n",
    "\n",
    "    # Increment epoch\n",
    "    epoch+=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [
    "# Create empty list for predictions\n",
    "test_predictions = []\n",
    "# Iterate through test data\n",
    "for image in test:\n",
    "    # Forward pass: compute Output/Prediction (o)\n",
    "    h1 = calculate_layer_output(w1, image.reshape(image.shape[0], 1)\n",
    "                                                , b1, activation_type=\"relu\")\n",
    "    h2 = calculate_layer_output(w2, h1, b2, activation_type=\"relu\")\n",
    "    o = calculate_layer_output(w3, h2, b3, activation_type=\"softmax\")\n",
    "\n",
    "    # Add prediction to list\n",
    "    test_predictions += [get_prediction(o)]\n",
    "\n",
    "# Write test predictions to submission.csv file\n",
    "submission = pd.DataFrame(test_predictions).reset_index().rename(\n",
    "                                        columns={\"index\":\"ImageId\",0:\"Label\"})\n",
    "submission['ImageId'] = submission['ImageId'] + 1\n",
    "submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "# helper function to show image\n",
    "from matplotlib import pyplot as plt\n",
    "def plot_image(x):\n",
    "    image = x.reshape(28, 28) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(image, interpolation='nearest')\n",
    "    plt.show()\n",
    "plot_image(train[8])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
